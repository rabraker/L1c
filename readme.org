#+TITLE: l1C: Compressed Sensing in c
#+SETUPFILE: ~/.emacs.d/org-templates/level-0.org
#+PROPERTY: TEMPLATE page_math
#+PROPERTY: URL projects/l1c.html 
#+PROPERTY: SAVE_AS projects/l1c.html
#+OPTIONS: tex:t
#+STARTUP: latexpreview
#+TODO: TODO(t) WAITING(w@/!) | DONE(d@/!) CANCELED(c@/!) STARTED(s@/!) DEFERRED(ef@/!)
#+SEQ_TODO: TODO STARTED WAITING DELEGATED APPT | DONE DEFERRED CANCELLED



* Introduction
The goal of this project is to provide a c library that is useful to compressed sensing.  To date, lots of research has been produced that develops methods to efficiently solve solve various formulations of the compressed sensing problem. To their great credit, many of those reasearchers have released code along with their publications. See, for example, [[https://statweb.stanford.edu/~candes/l1magic/][l1-magic]], [[http://statweb.stanford.edu/~candes/nesta/][NESTA]], [[https://sparselab.stanford.edu/][SparseLab]], and [[https://www.ece.rice.edu/~tag7/Tom_Goldstein/Split_Bregman.html][Split Bregman]]. So, a good question is, why re-do it in c?

- Much of the published code is written in Matlab. This means if you want to use the code in a Python or R or Julia or a c/c++ project, you must translate the Matlab code to your language. By contrast, c is something like the langua franca of computing. It is relatively straigtforward to interface c code with most languages, including Matlab.

- CS optimizations are computationally intensive. Thus, it seems beneficial to write optimized implementations of those optimizations and c fits that requirement (Fortran might be better, but I have forgotten it).

- c is fun (probably the main reason)

 
Presently, the only capability this library provides is to solve the problem

\begin{aligned}
\min_{x} ||x||_{1}  \quad \text{s.t.} \quad ||Ax -b||_{2} < \epsilon,
\end{aligned}

which, is mostly a port of ~l1qc_log_barrier.m~ from the ~l1-magic~ code, [[sec:l1qc_mod][with a few modifications]]. Currently, the code *only* operates in large-scale mode, which means that the transformation $Ax$ (and the adjoint, $A^Ty$) are performed by user supplied functions, rather than an explicit matrix-vector multiplication. This also means that solving for the descent direction is always done via conjugate gradients.

* Performance
So far, using ~l1C~ gives me a speed increase of between 2 and 5 times faster compared to the original matlab code, depending on which computer I run it on.

If you compile with FFTW+OpenBlas, it is important that both libraries are compiled with openmp. I dont quite understand what happens, but if this is not the case, performance can suffer dramatically. In addition, if you have a CPU with hyperthreading, it is important to export the environmental variable

~>> export OMP_NUM_THREADS=N~

where N is the number of *physical* cores (essentially, if you have HT, this is half the the number of processors you see in a resource monitor). The code currently can not detect this, and for number crunching applications like this one, HT is detrimental.

Setting ~OMP_BIND_PROC=true~ seems to cost us about 1 second.

* Building
This project uses the GNU Autotools build system. Considering that we are still at "version 0.0", there is not a release tarball yet, so you will need Autotools installed. I typically test on Debian Linux. I have also succesfully built the project on Windows 7 with MinGW64 (though not with MKL or mex support, yet).

The four steps are 
#+BEGIN_SRC bash
git clone git@github.com:rabraker/l1c.git l1c && cd l1c
./bootsrap                   # requires autotools
./configure [--with-fftw3 [--with-fftw3-threads=[ARG]]] [--with-mkl] [--with-mex]
make && make install
#+END_SRC

Running ~./configure --help~ will give you a list of options. It will probably be necessary, however, to augment the ~./configure~ step with the right environment variables. 

** Dependencies
Running the full build requires the following dependencies
- [[https://github.com/xianyi/OpenBLAS][OpenBlas]] or [[http://math-atlas.sourceforge.net/][ATLAS]]
- [[http://fftw.org/][FFTW3]]
- [[https://github.com/libcheck/check][libcheck]] (required for unit tests)
- [[https://github.com/DaveGamble/cJSON][cJSON]] (required for unit tests and example program). 
- GNU OpenMP (~libgomp~) 

OpenBlas/ATLAS and FFTW3 can be replaced with MKL. Details and suggestions follow. Or run ~./configure --help~ if this is old hat to you. 

*** Core Code
The core code with examples requires either (1) OpenBlas and FFTW3, (2) ATLAS and FFTW3, or (3) Intel's Math Kernel Library (MKL). Do not try to combine options (1) and (2) with option (3).
If OpenBlas/Atlas and FFTW3 are compiled with support for threading and the avx and sse instruction sets, these combinations have slightly better performance than MKL. It is likely desirable (certainly for ATLAS) to compile these yourself, since the libraries availible through your distribution may be older or not have been compiled with full optimization. For ATLAS, the compilation optimizes the binary for your specific computer, so it makes no sense to obtain a library built by somebody else. 

By default, the configure script will search for first OpenBlas, then ATLAS (MKL is handled as a special case, see below). If you have both installed, you can instruct ~configure~ to choose one over the other by setting the envirnmental variable ~BLAS_LIB=[openblas][satlas]~. The key requirement for the BLAS library is that it has the extension ~cblas_daxpby~, or in the case of ATLAS, ~catlas_daxpby~. Thus, in principle, you can use another BLAS library with this extension, though that is untested.

To enable compiling the examples, use the flag ~--with-fftw3~. FFtW3 threading comes in three flavors: (1) OpenMP threading (~libfftw_omp~), (2) POSIX threads (~libfftw3_threads~) and (3), as a single, combined library. To choose a particular threading version, use the flag ~--with-fftw3-threads=[combined][omp][threads][yes][no]~. The ~yes~ option will search over all three possibilities. 

To compile with MKL, use ~--with-mkl~. However, see below for [[sec:mkl_cautions][cautions regarding this option]].

To enable compiling a Matlab mex function, use ~--with-mex~. This requires that a script ~mexext~ is on your path, or that your export the environmental variable ~MEXEXT~ with the proper mex extension for your platform.

In general, it is likely (almost certain for mex) that these libraries will not be on your default search path, so you must tell ~configure~ where they are by defining them in the ~CPPFLAGS~ and ~LDFLAGS~ environmental variables. See the example below. 

*Note:* in general, the only use of the DCT is for the (user defined) $A*x$ and $A^{T}*y$ transformations. If your use case uses a different sort of transformation, then an FFT library is unecessary. If this is the case, then you can simply adapt the mex interface file to your needs. Running the tests would require you to modify them and supply a different transformation, and to eliminate the tests for ~dct.c~ and ~dct_mkl.c~.

*** Unit Tests
The unit tests depend on two additional libraries. Data for the unit tests is generated in Matlab for the more complicated functions. Data for simple routines is embedded within the test function itself (see, e.g., ~TEST_vcl.c~ and ~test_l1c_dlogsum~ in ~TEST_l1qc_newton.c~).
1. [[https://github.com/libcheck/check][libcheck]] is the unit testing framework. The makefile assumes the headers for check are located in 
 ~/usr/local/include~ and that libcheck.so is located in ~/usr/local/lib~.
3. Saving data in Matlab/octave is accomplished with [[https://github.com/fangq/jsonlab][jsonlab]].

It is highly recommended to install these dependencies and run the tests.

*** TODO Remove dependency on Matlab
It remains an outstanding goal to remove the dependency on Matlab for the test data.

*** Examples
On my machine, ~FFTW3~, ~cJSON~, and ~check~ are installed in ~/usr/local~. ATLAS is installed in ~/usr/local/ATLAS~ and OpenBlas is in ~/usr/local/openblas~. MKL is installed in ~/opt/intel/mkl~. Matlab is installed in ~/usr/local/MATLAB/R2018b/~. In general (with MATLAB being an exception), each of these directories should contain a ~lib~ and and ~include~ directory. You should point ~LDFLAGS~ to the lib directory and ~CPPFLAGS~ to the ~include~ directory. Thus, to configure with FFTW3 and OpenBlas, we run (as one line)
#+BEGIN_SRC bash
CPPFLAGS="-I/usr/local/include -I/usr/local/openblas/include" \
         LDFLAGS="-L/usr/local/lib -L/usr/local/openblas/lib" \
         ./configure --with-fftw3 \ --with-fftw3-threads=omp 
#+END_SRC


Alternatively, you export ~CPPFLAGS~ and ~LDFLAGS~:
#+BEGIN_SRC bash
export CPPFLAGS="-I/usr/local/include -I/usr/local/openblas/include"
export LDFLAGS="-L/usr/local/lib -L/usr/local/openblas/lib"
 ./configure --with-fftw3 --with-fftw3-threads=omp 
#+END_SRC

If we also we want to compile with the mex interface, we need to add Matlab's ~lib~ and ~include~ directories. These are in non-stanard locations:
#+BEGIN_SRC bash
export CPPFLAGS="-I/usr/local/include -I/usr/local/openblas/include \
                 -I/usr/local/MATLAB/R2018b/extern/include"
export LDFLAGS="-L/usr/local/lib -L/usr/local/openblas/lib  \
                -L/usr/local/MATLAB/R2018b/bin/glnxa64"
 ./configure --with-fftw3 --with-fftw3-threads=omp --with-mex
#+END_SRC

Note that on my system, the command ~mexext~ is located in ~/usr/local/MATLAB/R2018b/bin/~, which is symlinked to ~/usr/local/bin/mexext~, which is on my path. If this is not the case, then in addition to above you can, e.g., ~export MEXEXT=mexa64~. You can get the appropriate value to export by typing ~mexext~ at the matlab command prompt.


** Building the test data

1. ~build_log_barrier_test_data.m~: this scripts builds data to test the outer log-barrier iterations. JSON files are saved into test_data with the name ~sprintf('lb_test_data_iter_%d.json', lb_iter)~.
2. ~~build_newton_init_data.m~ Builds a small set of data to check that we compute the number of log-barrier iterations and tau parameter properly. Used in ~test_newton_init()~.

*** Cautions with MKL
<<sec:mkl_cautions>>

Building with MKL is finicky, especially if you want build a shared library that you use in python or matlab. Intel recommends using their  [[https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/][Link Advisor]] to figure out the right compiler flags and linking order. Unfortunately, this information is both incomplete and changes with different versions of MKL. I have had good success using the Link Advisor link line when compiling a stand alone program. When compiling a shared library that you then want to call from matlab (via either mex or dlopen) or from python, typically results in python and matlab being unable to resolve all the dependencies when they try to dlopen your library.

Especially on the python side, the internet is full of people complaining about undefined references (to, e.g., ~libmkl_avx~ and ~libmkl_def.so~) when trying to link Numpy with MKL [[[https://stackoverflow.com/questions/36659453/intel-mkl-fatal-error-cannot-load-libmkl-avx2-so-or-libmkl-def-so][1]]] [[[https://software.intel.com/en-us/forums/intel-math-kernel-library/topic/473447][2]]] [[[https://software.intel.com/en-us/forums/intel-math-kernel-library/topic/733207][3]]], with folks suggestion solutions like wrapper scripts that LD_PRELOAD the missing libaries. Even the Intel folks don't know exactly what to do with it. For me, I have had success adding the un-resolved libraries to the link line (which Intel still has not noted in the Link Advisor). However, this seems to not work for some people, may cause problems on chips without avx instructions, and may or may not work with other versions of MKL (I have tested with 2019). 

Thus, support for MKL should be considered experimental at best. In any case, the open source libraries are faster (on the two machines I've been able to test) and far easier to use.


* Usage
As a user, the primary function you need to worry about is
#+BEGIN_SRC c 

/*l1qc_newton.h */
extern LBResult l1qc_newton(l1c_int N, double *x, l1c_int M, double *b,
                            NewtParams params, AxFuns Ax_funs);

#+END_SRC

- ~int N~. The length of ~x~ and ~u~.
- ~double *x~. On entry, this should be an array of doubles length N, allocated on a 64-byte boundary (see below). On exit, x contains the result of the optimization.
- ~double *u~ On entry, this should contain an array with length N. On exit, it will contain the auxilary u (See above about the conversion from an l1 optimization to a linear program).
- ~int M~. The length b.
- ~double *b~. On entry, contains the 'measured data' (see above). In general, we expect M <N.
- ~NewtParams params~ is a struct containing parameters (e.g., tolerances and iteration number bounds). Will be described fully below.
- ~AxFuns Ax_funs~ is a struct containing pointers to the functions which perform the transformations.


*Important*: The array inputs of doubles (*x, *u, *b) to ~l1qc_newton~ must be aligned on a 64-byte boundary, otherwise segfaults may occur. To faciliate this, you may use the functions 

#+BEGIN_SRC c 
/*l1c_common.h */
void* malloc_double(N);
void* free_double(N);
#+END_SRC
The function ~malloc_double(N)~ will allocate memory for ~N~ doubles, aligned on a 64-byte boundary and ~free_double~ will free it.


The data structures are defined as
#+BEGIN_SRC c
//l1qc_newton.h
typedef struct LBResult{
  double l1;                // Final value of functional, ||x||_1
  int    total_newton_iter; // Total number of newton iterations.
  int    status;            // 0 if completed with no errors, 1 otherwise

}LBResult;

typedef struct NewtParams{
  double epsilon;
  double tau;
  double mu;
  double newton_tol;
  int newton_max_iter;
  int lbiter;
  double lbtol;
  int verbose;
  CgParams cg_params;

}NewtParams;

typedef struct AxFuns {
  void(*Ax)(double *x, double *y);
  void(*Aty)(double *y, double *x);
  void(*AtAx)(double *x, double *z);
}AxFuns;
#+END_SRC

The struct ~AxFuns~ contains pointers to your user-defined functions which compute $Ax$ and $A^{T}y$. For an example, see the mex-interface file ~l1qc_mex.c~ (in ~interfaces/~) and either ~dct.c~ or ~dct_mkl.c~. Note that although the mex interface looks long and complicated, almost all of this is boiler-plate parsing of Matlab's input to the function. The amount of code to modify for a different set of transform functions is only a few lines.

* Modifications from the ~l1-magic~ algorithm
<<sec:l1qc_mod>>
I have made a few changes (improvements?) to the original ~~l1-magic~ algorithm, both pertaining to the line search. These changes address issues with numerical, rather than mathematical, problems. As the ~l1-magic~ authors note, in the later stages of the optimziation, numerical difficulties arise and the line search can fail. These modifications help to push that point into the future, enabling more iterations.

1. In the original code, I noticed that at some point, the data become complex when it should have been purely real. One of the places where this occures is in the code which computes the maximum step-size which still satisfies the constraints (i.e., lines XX in the original code). In particular, the code which computes the largest number $s$ such such that, for $x_{k+1}= x_{k} + sd_{x_k}$, $||Ax_{k+1}-b||<\epsilon$ still holds. To do this, we expand into a scalar equation quadratic in $s$
   \begin{align}
   ||A(x+sd_{x})-b||^{2} - \epsilon^{2} &=0 \\
   s^{2}(d_x^{T}A^{T}Ad_x) + 2r^{T}Ad_x + r^{T}r - \epsilon^{2} &= 0\\
   \end{align}

   where $r = Ax - b$. Although the roots should always be real, due to either computing $d_{x}$ with insufficient accuracy (which accomplished via conjugate gradient) or otherwise, the roots become complex in the later stages. In matlab, the promation to a complex type happens silently and we start optimizing complex data, which is undersirable. In c, the ~sqrt~ operation simply returns NaN, which is also undersirable. When this happens, the modification is to set $s=1$ and let the line search deal with. This will work fine in c because taking the log of a negative number results in NaN. In Matlab, we need something like ~log(max(0, a))~.

2. The goal of the line-search is to find (approximitaly) the largest step-size $s$ such that
   \begin{equation}
   f(z + sd_{z}) < f(z) + \alpha s \nabla f\cdot d_{z}
   \end{equation}
   In the original code, the functional $f(z)$ is only evaluated explicitly at the start of each log-barrier iteration and the value of $f(z_{i})$ is updated from derived values, e.g., $r_{k+1}= r_{k} + sAd_{x}$. Mathematically, this is fine. Numerically, it is problematic because after enough iterations the explicit value of $f(z_{k})$ becomes infinite (due to the barrier functions) even though the putative value is finite. Thus, although it is less efficient, this code evaluates the functional explicitly at each iteration of the line-search and this value is then passed to the next Newton iteration.

* To-Dos
1. Enable detection hyperthreading, and set ~omp_num_threads~ to half the number of reported cores.
2. Figure out the license. This may mean re-working all the test code because ~l1-magic~ doesn't come with an explicit license.
3. Make it build on windows, at least using cygwin or mingw.
4. Other optimization routines. On the list are
   - The TV-denoising problem using Bregman iteration. This seems like a really nice way to reduce much of the noise CS reconstructions.
   - [[https://web.stanford.edu/~boyd/l1_ls/][l1-ls]] from Stephen Boyd's research group.
   - NESTA, which from my cursory inspection, seems to depend on l1-ls.

5. Generalize the backtracking line search. There is really no reason that it needs to be specific to the l1qc algorithm. All it needs is a way to evaluate the functional and gradient at different step sizes.

6. With a bit of work, it should be possible to generalize the entire set of log-barrier and newton iterations, so that it is not specific the quadratically constrained l1 problem. Basically, all that is required is
   - A function to evaluate the functional
   - A function to compute the descent direction
   - A function to compute the linear approximation for the linesearch
   - A function to compute the max-step size. This seems like the main difference to a standard Newton descent algorithm and this one with barrier functions.
   - A function to compute the stopping criteria.

